###BUILD the corpus###
docs<-Corpus(VectorSource(docs))
docs<-tm_map(docs,content_transformer(tolower))
###CLEAN the data###
removeNumPunct<-function(x) gsub("[^[:alpha:][:space:][:alnum:][^a-zA-Z0-9]]*","",x)
docs<-tm_map(docs, content_transformer(removeNumPunct))
docs<-tm_map(docs, stripWhitespace)
docs<-tm_map(docs, removeWords, stopwords("english"))
docs<-tm_map(docs, removeNumbers)
docs<-tm_map(docs, gsub, pattern="country<e3><e2><e2>s",replace="country")
docs<-tm_map(docs, gsub, pattern="region<e3><e2><e2>s",replace="region")
docs<-tm_map(docs, gsub, pattern="government<e3><e2><e2>s",replace="government")
docs<-tm_map(docs, gsub, pattern="people<e3><e2><e2>s",replace="people")
docs<-tm_map(docs, gsub, pattern="month<e3><e2><e2>s",replace="month")
docs<-tm_map(docs, gsub, pattern="visayas<e3><e2><e2>",replace="visayas")
docs<-tm_map(docs, gsub, pattern="communities<e3><e2><e2>",replace="communities")
docs<-tm_map(docs, gsub, pattern="leyte<e3><e2><e2>s",replace="leyte")
docs<-tm_map(docs, gsub, pattern="growth<e3><e2><e2>",replace="growth")
docs<-tm_map(docs, gsub, pattern="sell<e3><e2><e2>it",replace="sell")
docs<-tm_map(docs, gsub, pattern="towns<e3><e2><e2>",replace="towns")
docs<-tm_map(docs, gsub, pattern="water<e3><e2><e2>",replace="water")
docs<-tm_map(docs, gsub, pattern="<e3><e2><e5>extreme",replace="extreme")
docs<-tm_map(docs, gsub, pattern="<e3><e2><e5>imperial",replace="imperial")
docs<-tm_map(docs, gsub, pattern="<e3><e2><e5>tubig",replace="water")
docs<-tm_map(docs, gsub, pattern="seaman<e3><e2><e2>",replace="seaman")
docs<-tm_map(docs, gsub, pattern="manila<e3><e2><e2>",replace="manila")
docs<-tm_map(docs, gsub, pattern="challenge<e3><e2><e2>",replace="challenge")
docs<-tm_map(docs, removeWords,c("<e3><e2><e5>we",
"also",
"<e3><e2><e5>this",
"<e3><e2><e5>minsan",
"challenge",
"kami<e3><e2><e2>",
"<e3><e2><e5>sana",
"doj<e3><e2><e2>",
"can<e3><e2><e2>t",
"<e3><e2><e5>basta",
"<e3><e2><e5>",
"will",
"said",
"among",
"first",
"we<e3><e2><e2>d"))
###create a Term DOcument Matrix###
tdm<-TermDocumentMatrix(docs)
tdm<-as.matrix(tdm)
#### Create Bar Plot/Graph###
rs<-rowSums(tdm)
rs<-sort(rowSums(tdm), decreasing = TRUE)
write.csv(rs,"GreenCompTDM.csv")
rs<-subset(rs, rs>44)
x<-barplot(rs,
las = 2,
col = rainbow(50),
beside = TRUE,
legend.text = TRUE,
args.legend = list(bty="n", horiz=TRUE),
ylim = c(0, 200),
ylab = "Frequency",
main = "Top Words")
y<-as.matrix(rs)
text(x,y+2, labels = as.character(y))
###word Cloud###
rs<-sort(rowSums(tdm), decreasing = TRUE)
set.seed(1234)
wordcloud(words = names(rs),freq = rs,
random.order = F,
max.words = 100,
min.freq = 15,
colors = brewer.pal(8, "Dark"),
rot.per = .1)
###Text Mining###
tdm<-TermDocumentMatrix(docs)
m<-as.matrix(tdm)
v<-sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word = names(v), freq=v)
head(d, 10)
findFreqTerms(tdm, 200)
corr_p<-findAssocs(tdm, terms = "Green Computing", corlimit = 0.6)
corr_p
write.csv(corr_p,"corr_p.csv")
corr_per<-findAssocs(tdm, terms = "percent", corlimit = 0.6)
###Sentiment ANalysis###
GreenComp<-read_excel("Green Computing.xlsx")
tw<-iconv(GreenComp$Text, to = "utf-8")
s<-get_nrc_sentiment(tw)
write.csv(s, "GCSenti.csv")
head(s)
###Examine TExt###
get_nrc_sentiment("percent")
get_nrc_sentiment("Green Computing")
###Sentiment Bar Plot###
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = "Frequency",
main = "Sentiment Scores")
library(RSentiment)
library(tm)
library(SentimentAnalysis)
library(syuzhet)
(RSentiment)
(RSentiment)
(RSentiment)
(RSentiment)
(RSentiment)
(RSentiment)
(RSentiment)
(RSentiment)
library(RSentiment)
library(tm)
library(SentimentAnalysis)
library(syuzhet)
library(readxl)
library(dplyr)
library(stringr)
library(ggplot2)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(igraph)
library(servr)
library(scales)
library(reshape2)
library(lubridate)
library(twitteR)
library(RCurl)
library(wordcloud2)
library(tidyverse)
library(tidytext)
library(lda)
library(topicmodels)
library(textdata)
library(tidyr)
setwd("C:\\Users\\Jules\\Desktop\\PSITE6(2)\\final")
GreenComp<-read_excel("Green Computing.xlsx")
docs<-iconv(GreenComp$Text, to = "utf-8")
###BUILD the corpus###
docs<-Corpus(VectorSource(docs))
docs<-tm_map(docs,content_transformer(tolower))
###CLEAN the data###
removeNumPunct<-function(x) gsub("[^[:alpha:][:space:][:alnum:][^a-zA-Z0-9]]*","",x)
docs<-tm_map(docs, content_transformer(removeNumPunct))
docs<-tm_map(docs, stripWhitespace)
docs<-tm_map(docs, removeWords, stopwords("english"))
docs<-tm_map(docs, removeNumbers)
docs<-tm_map(docs, gsub, pattern="country<e3><e2><e2>s",replace="country")
docs<-tm_map(docs, gsub, pattern="region<e3><e2><e2>s",replace="region")
docs<-tm_map(docs, gsub, pattern="government<e3><e2><e2>s",replace="government")
docs<-tm_map(docs, gsub, pattern="people<e3><e2><e2>s",replace="people")
docs<-tm_map(docs, gsub, pattern="month<e3><e2><e2>s",replace="month")
docs<-tm_map(docs, gsub, pattern="visayas<e3><e2><e2>",replace="visayas")
docs<-tm_map(docs, gsub, pattern="communities<e3><e2><e2>",replace="communities")
docs<-tm_map(docs, gsub, pattern="leyte<e3><e2><e2>s",replace="leyte")
docs<-tm_map(docs, gsub, pattern="growth<e3><e2><e2>",replace="growth")
docs<-tm_map(docs, gsub, pattern="sell<e3><e2><e2>it",replace="sell")
docs<-tm_map(docs, gsub, pattern="towns<e3><e2><e2>",replace="towns")
docs<-tm_map(docs, gsub, pattern="water<e3><e2><e2>",replace="water")
docs<-tm_map(docs, gsub, pattern="<e3><e2><e5>extreme",replace="extreme")
docs<-tm_map(docs, gsub, pattern="<e3><e2><e5>imperial",replace="imperial")
docs<-tm_map(docs, gsub, pattern="<e3><e2><e5>tubig",replace="water")
docs<-tm_map(docs, gsub, pattern="seaman<e3><e2><e2>",replace="seaman")
docs<-tm_map(docs, gsub, pattern="manila<e3><e2><e2>",replace="manila")
docs<-tm_map(docs, gsub, pattern="challenge<e3><e2><e2>",replace="challenge")
docs<-tm_map(docs, removeWords,c("<e3><e2><e5>we",
"also",
"<e3><e2><e5>this",
"<e3><e2><e5>minsan",
"challenge",
"kami<e3><e2><e2>",
"<e3><e2><e5>sana",
"doj<e3><e2><e2>",
"can<e3><e2><e2>t",
"<e3><e2><e5>basta",
"<e3><e2><e5>",
"will",
"said",
"among",
"first",
"we<e3><e2><e2>d"))
###create a Term DOcument Matrix###
tdm<-TermDocumentMatrix(docs)
tdm<-as.matrix(tdm)
#### Create Bar Plot/Graph###
rs<-rowSums(tdm)
rs<-sort(rowSums(tdm), decreasing = TRUE)
write.csv(rs,"GreenCompTDM.csv")
rs<-subset(rs, rs>44)
x<-barplot(rs,
las = 2,
col = rainbow(50),
beside = TRUE,
legend.text = TRUE,
args.legend = list(bty="n", horiz=TRUE),
ylim = c(0, 200),
ylab = "Frequency",
main = "Top Words")
y<-as.matrix(rs)
text(x,y+2, labels = as.character(y))
###word Cloud###
rs<-sort(rowSums(tdm), decreasing = TRUE)
set.seed(1234)
wordcloud(words = names(rs),freq = rs,
random.order = F,
max.words = 100,
min.freq = 15,
colors = brewer.pal(8, "Dark"),
rot.per = .1)
###Text Mining###
tdm<-TermDocumentMatrix(docs)
m<-as.matrix(tdm)
v<-sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word = names(v), freq=v)
head(d, 10)
findFreqTerms(tdm, 200)
corr_p<-findAssocs(tdm, terms = "Green Computing", corlimit = 0.6)
corr_p
write.csv(corr_p,"corr_p.csv")
corr_per<-findAssocs(tdm, terms = "percent", corlimit = 0.6)
###Sentiment ANalysis###
GreenComp<-read_excel("Green Computing.xlsx")
tw<-iconv(GreenComp$Text, to = "utf-8")
s<-get_nrc_sentiment(tw)
write.csv(s, "GCSenti.csv")
head(s)
###Examine TExt###
get_nrc_sentiment("percent")
get_nrc_sentiment("Green Computing")
###Sentiment Bar Plot###
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = "Frequency",
main = "Sentiment Scores")
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(quanteda); library(tidyverse); library(RColorBrewer)
dataset <- read_csv("./articles-sample.csv")
#create corpus
myCorpus <- corpus(dataset$Abstract)
dfm <- dfm(myCorpus,
remove = c(stopwords("english")),
ngrams=1L,
stem = F,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
vdfm <- dfm_trim(dfm, min_count = 10, min_docfreq = 5)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs
topfeatures(vdfm, n = 50)
textplot_wordcloud(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"),
random.order = F, rot.per=0.1, max.words=250, main = "Raw Counts")
textplot_wordcloud(tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"),
random.order = F, rot.per=0.1, max.words=250, main = "TF-IDF")
numWords <- 50
wordDfm <- dfm_sort(dfm_weight(vdfm, "tfidf"))
wordDfm <- t(wordDfm)[1:numWords,]  # keep the top numWords words
wordDistMat <- dist(wordDfm)
wordCluster <- hclust(wordDistMat)
plot(wordCluster, xlab="", main="TF-IDF Frequency weighting (First 50 Words)")
library(topicmodels)
# we now export to a format that we can run the topic model with
dtm <- convert(vdfm, to="topicmodels")
# estimate LDA with K topics
K <- 20
lda <- LDA(dtm, k = K, method = "Gibbs",
control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
#Create Json for LDAVis
library(LDAvis)
source('./functions.R')
json <- topicmodels_json_ldavis(lda,vdfm,dtm)
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(quanteda); library(tidyverse); library(RColorBrewer)
dataset <- read_csv("./articles-greencomputing.csv")
#create corpus
myCorpus <- corpus(dataset$Abstract)
stopwords1 <- c("green")
dfm <- dfm(myCorpus,
remove = c(stopwords("english"), stopwords1),
ngrams=1L,
stem = F,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
)
# stopwords removed?
a <- featnames(dfm)
lapply(stopwords1, function(x){
a[str_detect(a, x)]
})
vdfm <- dfm_trim(dfm, min_count = 1, min_docfreq = 1)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs
topfeatures(vdfm, n = 50)
textplot_wordcloud(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"),
random.order = F, rot.per=0.1, max.words=250, main = "Raw Counts")
textplot_wordcloud(tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"),
random.order = F, rot.per=0.1, max.words=250, main = "TF-IDF")
numWords <- 5
wordDfm <- dfm_sort(dfm_weight(vdfm, "tfidf"))
wordDfm <- t(wordDfm)[1:numWords,]  # keep the top numWords words
wordDistMat <- dist(wordDfm)
wordCluster <- hclust(wordDistMat)
plot(wordCluster, xlab="", main="TF-IDF Frequency weighting (First 5 Words)")
library(topicmodels)
# we now export to a format that we can run the topic model with
dtm <- convert(vdfm, to="topicmodels")
# estimate LDA with K topics
K <- 50
lda <- LDA(dtm, k = K, method = "Gibbs",
control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
#Create Json for LDAVis
library(LDAvis)
source('./functions.R')
json <- topicmodels_json_ldavis(lda,vdfm,dtm)
new.order <- RJSONIO::fromJSON(json)$topic.order
# change open.browser = TRUE to automatically open result in browser
serVis(json, out.dir = 'unccResearch', open.browser = F)
term <- terms(lda, 10)
# Topic #'s reordered!!
term <- term[,new.order]
colnames(term) <- paste("Topic",1:K)
term
# to get topic probabilities per document
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics)
probtopics <- probtopics[,new.order]
colnames(probtopics) <- paste("Topic",1:K)
filter.topic <- "Topic 1"
row <- order(-probtopics[,filter.topic])[1]
dataset$Abstract[row]
sessionInfo()
term <- terms(lda, 10)
# Topic #'s reordered!!
term <- term[,new.order]
colnames(term) <- paste("Topic",1:K)
term
# to get topic probabilities per document
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics)
probtopics <- probtopics[,new.order]
colnames(probtopics) <- paste("Topic",1:K)
filter.topic <- "Topic 1"
row <- order(-probtopics[,filter.topic])[1]
dataset$Abstract[row]
sessionInfo()
# to get topic probabilities per document
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics)
probtopics <- probtopics[,new.order]
colnames(probtopics) <- paste("Topic",1:K)
filter.topic <- "Topic 1"
row <- order(-probtopics[,filter.topic])[1]
dataset$Abstract[row]
sessionInfo()
numWords <- 50
wordDfm <- dfm_sort(dfm_weight(vdfm, "tfidf"))
wordDfm <- t(wordDfm)[1:numWords,]  # keep the top numWords words
wordDistMat <- dist(wordDfm)
wordCluster <- hclust(wordDistMat)
plot(wordCluster, xlab="", main="TF-IDF Frequency weighting (First 5 Words)")
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(quanteda); library(tidyverse); library(RColorBrewer); library(stm)
dataset <- read_csv("./articles-sample.csv")
#create corpus
myCorpus <- corpus(dataset$Abstract)
stopWords <- c("can","use","uses","used","using","study","studies","first","second","third","also","across","results","result","resulted","may","however","one","two","three","four","five","among","well","within","many","related","i.e","e.g","find","finding","finds","found","increase","increases","increasing","increased","decreased","decrease","decreases","decreasing","propose","proposal","proposals","proposes","proposed","new","old","differ","differs","different","difference","differences","positive","negative","findings","reports","report","reported","state","states","article","articles","examines","examine","suggest","research","researches","researchers","need","needs","show","shows","association","associations","associated","discuss","discusses","discussed","will","likely","unlikely","paper","method","methods","methodology","compared","specifically","approach","impact","impacts","examine","examined","examines","includes","include","included","including","measure","measures","measured","analysis","analyze","analyses","complete","completes","completed","indicate","indicated","indicates","high","higher","low","lower","follow","follows","following","significant","significance","approach","approaches","approached","model","models","demonstrate","demonstrated","demonstrates","yet","best","worst","better","large","small","larger","smaller","several","few","much","less","given","via","long","short","often","years","along","whether","potential","significantly","influence","influenced","influences","develop","develops","developed","good","bad","based","p","group","groups","effect","affect","affects","effects","sample","samples","relationship","relationships","change","changes","m","k","conclusion","conclusions","present","presents")
dfm <- dfm(myCorpus,
remove = c(stopwords("english"), stopWords),
ngrams= 1L,
stem = F,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
vdfm <- dfm_trim(dfm, min_count = 10, min_docfreq = 5)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs
topfeatures(vdfm, n = 50)
plot(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"),
random.order = F, rot.per=0.1, max.words=250, main = "Raw Counts")
plot(tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"),
random.order = F, rot.per=0.1, max.words=250, main = "TF-IDF")
numWords <- 50
wordDfm <- sort(weight(vdfm, "tfidf"))
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(quanteda); library(tidyverse); library(RColorBrewer); library(stm)
dataset <- read_csv("./articles-sample.csv")
#create corpus
myCorpus <- corpus(dataset$Abstract)
docvars(myCorpus, field = "Subject") <- ifelse(dataset$College=="Computing and Informatics",
"Computing","Social Science")
docvars(myCorpus, field = "Year") <- as.integer(dataset$Year)
stopWords <- c("can","use","uses","used","using","study","studies","first","second","third","also","across","results","result","resulted","may","however","one","two","three","four","five","among","well","within","many","related","i.e","e.g","find","finding","finds","found","increase","increases","increasing","increased","decreased","decrease","decreases","decreasing","propose","proposal","proposals","proposes","proposed","new","old","differ","differs","different","difference","differences","positive","negative","findings","reports","report","reported","state","states","article","articles","examines","examine","suggest","research","researches","researchers","need","needs","show","shows","association","associations","associated","discuss","discusses","discussed","will","likely","unlikely","paper","method","methods","methodology","compared","specifically","approach","impact","impacts","examine","examined","examines","includes","include","included","including","measure","measures","measured","analysis","analyze","analyses","complete","completes","completed","indicate","indicated","indicates","high","higher","low","lower","follow","follows","following","significant","significance","approach","approaches","approached","model","models","demonstrate","demonstrated","demonstrates","yet","best","worst","better","large","small","larger","smaller","several","few","much","less","given","via","long","short","often","years","along","whether","potential","significantly","influence","influenced","influences","develop","develops","developed","good","bad","based","p","group","groups","effect","affect","affects","effects","sample","samples","relationship","relationships","change","changes","m","k","conclusion","conclusions","present","presents")
dfm <- dfm(myCorpus,
remove = c(stopwords("english"), stopWords),
ngrams= 1L,
stem = F,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
library(stm)
# use quanteda converter to convert our Dfm
stmdfm <- convert(dfm, to = "stm", docvars = docvars(myCorpus))
plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))
out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)
k <- 40
load("./stmFit.RData")
#stmFit <- stm(out$documents, out$vocab, K = k, prevalence =~ s(Year) + Subject,
#              max.em.its = 150, data = out$meta, init.type = "Spectral", seed = 300)
#save(stmFit, file = "./stmFit.RData")
plot(stmFit,
type = "summary",
xlim = c(0,.16),
n = 5,
labeltype = "prob",
main = "UNCC Research Topics",
text.cex = 0.8)
topicNames <- labelTopics(stmFit, n = 5)
topic <- data.frame(
TopicNumber = 1:k,
TopicProportions = colMeans(stmFit$theta))
prep <- estimateEffect(1:k ~ Subject + s(Year), stmFit, meta = out$meta, uncertainty = "Global")
# time
par(mfrow = c(1, 1), mar = c(4, 4, 2, 2))
i <- c(9, 18)
plot(
prep,
"Year",
method = "continuous",
topics = i,
main = "Topics 9 and 18 by Year",
printlegend = T,
ylab = "Exp. Topic Prob",
xlab = "Year",
ylim = c(-0.01, 0.16)
)
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(quanteda); library(tidyverse); library(RColorBrewer); library(stm)
dataset <- read_csv("./articles-sample.csv")
#create corpus
myCorpus <- corpus(dataset$Abstract)
docvars(myCorpus, field = "Subject") <- ifelse(dataset$College=="Computing and Informatics",
"Computing","Social Science")
docvars(myCorpus, field = "Year") <- as.integer(dataset$Year)
stopWords <- c("can","use","uses","used","using","study","studies","first","second","third","also","across","results","result","resulted","may","however","one","two","three","four","five","among","well","within","many","related","i.e","e.g","find","finding","finds","found","increase","increases","increasing","increased","decreased","decrease","decreases","decreasing","propose","proposal","proposals","proposes","proposed","new","old","differ","differs","different","difference","differences","positive","negative","findings","reports","report","reported","state","states","article","articles","examines","examine","suggest","research","researches","researchers","need","needs","show","shows","association","associations","associated","discuss","discusses","discussed","will","likely","unlikely","paper","method","methods","methodology","compared","specifically","approach","impact","impacts","examine","examined","examines","includes","include","included","including","measure","measures","measured","analysis","analyze","analyses","complete","completes","completed","indicate","indicated","indicates","high","higher","low","lower","follow","follows","following","significant","significance","approach","approaches","approached","model","models","demonstrate","demonstrated","demonstrates","yet","best","worst","better","large","small","larger","smaller","several","few","much","less","given","via","long","short","often","years","along","whether","potential","significantly","influence","influenced","influences","develop","develops","developed","good","bad","based","p","group","groups","effect","affect","affects","effects","sample","samples","relationship","relationships","change","changes","m","k","conclusion","conclusions","present","presents")
dfm <- dfm(myCorpus,
remove = c(stopwords("english"), stopWords),
ngrams= 1L,
stem = F,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
library(stm)
# use quanteda converter to convert our Dfm
stmdfm <- convert(dfm, to = "stm", docvars = docvars(myCorpus))
plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))
out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)
k <- 40
load("./stmFit.RData")
#stmFit <- stm(out$documents, out$vocab, K = k, prevalence =~ s(Year) + Subject,
#              max.em.its = 150, data = out$meta, init.type = "Spectral", seed = 300)
#save(stmFit, file = "./stmFit.RData")
plot(stmFit,
type = "summary",
xlim = c(0,.16),
n = 5,
labeltype = "prob",
main = "UNCC Research Topics",
text.cex = 0.8)
topicNames <- labelTopics(stmFit, n = 5)
topic <- data.frame(
TopicNumber = 1:k,
TopicProportions = colMeans(stmFit$theta))
prep <- estimateEffect(1:k ~ Subject + s(Year), stmFit, meta = out$meta, uncertainty = "Global")
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(quanteda); library(tidyverse); library(RColorBrewer); library(stm)
dataset <- read_csv("./articles-sample.csv")
#create corpus
myCorpus <- corpus(dataset$Abstract)
docvars(myCorpus, field = "Subject") <- ifelse(dataset$College=="Computing and Informatics",
"Computing","Social Science")
docvars(myCorpus, field = "Year") <- as.integer(dataset$Year)
stopWords <- c("can","use","uses","used","using","study","studies","first","second","third","also","across","results","result","resulted","may","however","one","two","three","four","five","among","well","within","many","related","i.e","e.g","find","finding","finds","found","increase","increases","increasing","increased","decreased","decrease","decreases","decreasing","propose","proposal","proposals","proposes","proposed","new","old","differ","differs","different","difference","differences","positive","negative","findings","reports","report","reported","state","states","article","articles","examines","examine","suggest","research","researches","researchers","need","needs","show","shows","association","associations","associated","discuss","discusses","discussed","will","likely","unlikely","paper","method","methods","methodology","compared","specifically","approach","impact","impacts","examine","examined","examines","includes","include","included","including","measure","measures","measured","analysis","analyze","analyses","complete","completes","completed","indicate","indicated","indicates","high","higher","low","lower","follow","follows","following","significant","significance","approach","approaches","approached","model","models","demonstrate","demonstrated","demonstrates","yet","best","worst","better","large","small","larger","smaller","several","few","much","less","given","via","long","short","often","years","along","whether","potential","significantly","influence","influenced","influences","develop","develops","developed","good","bad","based","p","group","groups","effect","affect","affects","effects","sample","samples","relationship","relationships","change","changes","m","k","conclusion","conclusions","present","presents")
dfm <- dfm(myCorpus,
remove = c(stopwords("english"), stopWords),
ngrams= 1L,
stem = F,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
library(stm)
# use quanteda converter to convert our Dfm
stmdfm <- convert(dfm, to = "stm", docvars = docvars(myCorpus))
plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))
out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)
k <- 40
load("./stmFit.RData")
#stmFit <- stm(out$documents, out$vocab, K = k, prevalence =~ s(Year) + Subject,
#              max.em.its = 150, data = out$meta, init.type = "Spectral", seed = 300)
#save(stmFit, file = "./stmFit.RData")
plot(stmFit,
type = "summary",
xlim = c(0,.16),
n = 5,
labeltype = "prob",
main = "UNCC Research Topics",
text.cex = 0.8)
topicNames <- labelTopics(stmFit, n = 5)
topic <- data.frame(
TopicNumber = 1:k,
TopicProportions = colMeans(stmFit$theta))
prep <- estimateEffect(1:k ~ Subject + s(Year), stmFit, meta = out$meta, uncertainty = "Global")
